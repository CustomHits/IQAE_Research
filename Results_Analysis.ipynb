{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b488667",
   "metadata": {},
   "source": [
    "# Please make sure you have ran MevrickDaCostaUCL_Quantum_Algorithms.py first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13ca2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46d1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.algorithms import IterativeAmplitudeEstimation, EstimationProblem\n",
    "from qiskit.circuit.library import LinearAmplitudeFunction\n",
    "from qiskit_finance.circuit.library import LogNormalDistribution\n",
    "from qiskit.utils import QuantumInstance\n",
    "\n",
    "from qiskit import IBMQ\n",
    "from qiskit.providers.ibmq import least_busy\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fa8d3",
   "metadata": {},
   "source": [
    "# Change the location to where you saved the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('disso results/c_approx_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_and_plot(data, independent_variable='c_approx'):\n",
    "    simulated_results = data.replace([float('inf'), float('-inf')], pd.NA).dropna()\n",
    "\n",
    "    aggregated_data = simulated_results.groupby(independent_variable).agg({\n",
    "        'accuracy_error': 'mean',\n",
    "        'computation_time': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_data['normalised_accuracy_error'] = (aggregated_data['accuracy_error'] - aggregated_data['accuracy_error'].min()) / \\\n",
    "                                                   (aggregated_data['accuracy_error'].max() - aggregated_data['accuracy_error'].min())\n",
    "    aggregated_data['normalised_computation_time'] = (aggregated_data['computation_time'] - aggregated_data['computation_time'].min()) / \\\n",
    "                                                     (aggregated_data['computation_time'].max() - aggregated_data['computation_time'].min())\n",
    "    \n",
    "    # Update the weighted_combined_cost with new weights\n",
    "    aggregated_data['weighted_combined_cost'] = (2 * aggregated_data['normalised_accuracy_error'] + aggregated_data['normalised_computation_time']) / 3\n",
    "\n",
    "    simulated_results['price_parameters'] = simulated_results['p'].apply(ast.literal_eval)\n",
    "    simulated_results['volatility'] = simulated_results['price_parameters'].apply(lambda x: x[1])\n",
    "    simulated_results['time_to_expiry'] = simulated_results['price_parameters'].apply(lambda x: x[3])\n",
    "    simulated_results = simulated_results.drop(columns=['price_parameters'])\n",
    "\n",
    "    simulated_results['normalised_accuracy_error'] = (simulated_results['accuracy_error'] - simulated_results['accuracy_error'].min()) / \\\n",
    "                                                      (simulated_results['accuracy_error'].max() - simulated_results['accuracy_error'].min())\n",
    "    simulated_results['normalised_computation_time'] = (simulated_results['computation_time'] - simulated_results['computation_time'].min()) / \\\n",
    "                                                        (simulated_results['computation_time'].max() - simulated_results['computation_time'].min())\n",
    "    simulated_results['weighted_combined_cost'] = (2 * simulated_results['normalised_accuracy_error'] + simulated_results['normalised_computation_time']) / 3\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    ax1 = axs[0]\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(aggregated_data[independent_variable], aggregated_data['accuracy_error'], 'g-')\n",
    "    ax2.plot(aggregated_data[independent_variable], aggregated_data['computation_time'], 'b-')\n",
    "    ax1.set_ylabel('Mean Accuracy Error', color='g')\n",
    "    ax2.set_ylabel('Mean Computation Time', color='b')\n",
    "    ax1.set_xlabel(independent_variable)\n",
    "    ax1.set_title(f'Relationship of {independent_variable} with Mean Accuracy Error and Mean Computation Time')\n",
    "\n",
    "    axs[1].plot(aggregated_data[independent_variable], aggregated_data['weighted_combined_cost'], '-o', label='Weighted Combined Cost')\n",
    "    axs[1].set_xlabel(independent_variable)\n",
    "    axs[1].set_ylabel('Weighted Combined Cost (Normalised)')\n",
    "    axs[1].set_title(f'Relationship of {independent_variable} with Weighted Combined Cost')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_and_plot(data, 'c_approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('disso results/epsilon_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57452f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_and_plot(data2, 'epsilon_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f40e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv('disso results/shots_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_and_plot(data3, 'shots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb952a7",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_result_df = pd.read_csv('disso results\\paramter_tuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b11120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameter_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_result_df.replace([float('inf'), float('-inf')], pd.NA).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a798c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuning =  pd.read_csv('disso results/paramter_tuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a733d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuning.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7826aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "data = parameter_tuning[['epsilon_target', 'shots', 'c_approx']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(data_scaled)\n",
    "    preds = kmeans.predict(data_scaled)\n",
    "    sil_score = silhouette_score(data_scaled, preds)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "optimal_clusters = sil_scores.index(max(sil_scores)) + 2  # +2 because our range starts from 2\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=64)\n",
    "parameter_tuning['cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "cluster_means = parameter_tuning.groupby('cluster').mean()[['c_approx', 'shots', 'epsilon_target', 'accuracy_error']]\n",
    "cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5da9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def black_scholes_call(S, K, T, r, sigma):\n",
    "    \n",
    "    d1 = (math.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * math.sqrt(T))\n",
    "    d2 = d1 - sigma * math.sqrt(T)\n",
    "    \n",
    "    call_price = S * norm.cdf(d1) - K * math.exp(-r * T) * norm.cdf(d2)\n",
    "    \n",
    "    return call_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cb1f5",
   "metadata": {},
   "source": [
    "### Creating dummy data -- keeping the contract characteristics constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afe2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerDict = {}\n",
    "\n",
    "S = 105\n",
    "tickerDict['RNDM'] = 105\n",
    "\n",
    "vals_volatility = [i/10000 for i in range(1000, 6500, 25)]\n",
    "T_constant = 150/365\n",
    "\n",
    "strike_prices = [105, 102.5, 100, 97.5]\n",
    "\n",
    "vol_arr = [(strike, vol, \"RNDM\", T_constant, black_scholes_call(S, strike, T_constant, 0.0527, vol)) \n",
    "       for strike in strike_prices for vol in vals_volatility]\n",
    "\n",
    "S = 105\n",
    "sigma_constant = 0.25 \n",
    "T_values = [i/365 for i in range(1, 186, 1)]\n",
    "\n",
    "strike_prices = [105, 102.5, 100, 97.5]\n",
    "\n",
    "time_arr = [(strike, sigma_constant, \"RNDM\", T, black_scholes_call(S, strike, T, 0.0527, sigma_constant)) \n",
    "       for strike in strike_prices for T in T_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_builder(params, c_approx):\n",
    "    num_uncertainty_qubits = 3\n",
    "    \n",
    "    r = 0.0527\n",
    "    \n",
    "    K, vol, ticker, T, _ = params\n",
    "\n",
    "    mu = (r - 0.5 * vol**2) * T + np.log(tickerDict[ticker])\n",
    "    sigma = vol * np.sqrt(T)\n",
    "    mean = np.exp(mu + sigma**2 / 2)\n",
    "    variance = (np.exp(sigma**2) - 1) * np.exp(2* mu + sigma**2)\n",
    "    stddev = np.sqrt(variance)\n",
    "    low = np.maximum(0, mean-3*stddev)\n",
    "    high = mean + 3 * stddev\n",
    "\n",
    "    uncertainty_model = LogNormalDistribution(num_uncertainty_qubits, mu=mu, sigma = sigma**2, bounds = (low, high))\n",
    "\n",
    "    breakpoints = [low, K]\n",
    "    slopes = [0, 1]\n",
    "    offsets = [0, 0]\n",
    "    f_min = 0\n",
    "    f_max = high - K\n",
    "    european_call_objective = LinearAmplitudeFunction(\n",
    "        num_uncertainty_qubits,\n",
    "        slopes,\n",
    "        offsets,\n",
    "        domain=(low, high),\n",
    "        image=(f_min, f_max),\n",
    "        breakpoints=breakpoints,\n",
    "        rescaling_factor=c_approx,\n",
    "    )\n",
    "\n",
    "\n",
    "    num_qubits = european_call_objective.num_qubits\n",
    "    european_call = QuantumCircuit(num_qubits)\n",
    "    european_call.append(uncertainty_model, range(num_uncertainty_qubits))\n",
    "    european_call.append(european_call_objective, range(num_qubits))\n",
    "\n",
    "    problem = EstimationProblem(\n",
    "        state_preparation=european_call,\n",
    "        objective_qubits=[3],\n",
    "        post_processing=european_call_objective.post_processing,\n",
    "    )\n",
    "\n",
    "    \n",
    "    return problem    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_parameters(data):\n",
    "    metrics, c_approx, epsilon_target, shots = data\n",
    "    print(f\"Starting evaluation for c_approx={c_approx}, epsilon_target={epsilon_target}, shots={shots}...\")    \n",
    "    \n",
    "    try:\n",
    "        problem = problem_builder(metrics, c_approx)\n",
    "    except ValueError:\n",
    "        print(f\"Encountered the breakpoint error for c_approx={c_approx}, epsilon_target={epsilon_target}, shots={shots}. Skipping this set of parameters.\")\n",
    "        return {\n",
    "            'p': metrics,\n",
    "            'c_approx': c_approx,\n",
    "            'epsilon_target': epsilon_target,\n",
    "            'shots': shots,\n",
    "            'accuracy_error': float('inf'),\n",
    "            'computation_time': float('inf'),\n",
    "            'cost': float('inf')\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    qi = QuantumInstance(backend=backend, shots=shots)\n",
    "    qae = IterativeAmplitudeEstimation(epsilon_target=epsilon_target, alpha=0.05, quantum_instance=qi)\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = qae.estimate(problem)\n",
    "    except AttributeError as e:\n",
    "        if \"NoneType\" in str(e):\n",
    "            print(f\"Encountered the NoneType error for c_approx={c_approx}, epsilon_target={epsilon_target}, shots={shots}. Skipping this set of parameters.\")\n",
    "            return {\n",
    "                'p': metrics,\n",
    "                'c_approx': c_approx,\n",
    "                'epsilon_target': epsilon_target,\n",
    "                'shots': shots,\n",
    "                'accuracy_error': float('inf'),\n",
    "                'computation_time': float('inf'),\n",
    "                'cost': float('inf')\n",
    "            }\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    computation_time = end_time - start_time\n",
    "    accuracy_error = np.abs(result.estimation_processed - metrics[-1])\n",
    "    cost_function =  computation_time + 1.2 * accuracy_error\n",
    "\n",
    "    print(f\"Completed evaluation for c_approx={c_approx}, epsilon_target={epsilon_target}, shots={shots}.\")\n",
    "\n",
    "    return {\n",
    "        'p': metrics,\n",
    "        'c_approx': c_approx,\n",
    "        'epsilon_target': epsilon_target,\n",
    "        'shots': shots,\n",
    "        'accuracy_error': float(accuracy_error),\n",
    "        'computation_time': computation_time,\n",
    "        'cost': float(cost_function)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f879e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "from qiskit import Aer\n",
    "\n",
    "backend = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fafd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for metrics in vol_arr:\n",
    "    c_approx = 0.237347\n",
    "    epsilon_target = 0.080041\n",
    "    shots = 6384\n",
    "    tasks.append((metrics, c_approx, epsilon_target, shots))\n",
    "\n",
    "print(\"Starting parameter tuning...\")\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(evaluate_parameters, tasks))\n",
    "\n",
    "print(\"Parameter tuning completed. Processing results...\")\n",
    "print(\"All tasks completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('sensitivity_vol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for metrics in time_arr:\n",
    "    c_approx = 0.237347\n",
    "    epsilon_target = 0.080041\n",
    "    shots = 6384\n",
    "    tasks.append((metrics, c_approx, epsilon_target, shots))\n",
    "\n",
    "print(\"Starting parameter tuning...\")\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(evaluate_parameters, tasks))\n",
    "\n",
    "print(\"Parameter tuning completed. Processing results...\")\n",
    "print(\"All tasks completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1158a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('sensitivty_T.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_averaged_data(data, independent_var_index):\n",
    "    \n",
    "    data = data[data['accuracy_error'] != np.inf]\n",
    "    \n",
    "    data['accuracy_error_normalised'] = (data['accuracy_error'] - data['accuracy_error'].min()) / (data['accuracy_error'].max() - data['accuracy_error'].min())\n",
    "    data['computation_time_normalised'] = (data['computation_time'] - data['computation_time'].min()) / (data['computation_time'].max() - data['computation_time'].min())\n",
    "\n",
    "    \n",
    "    data['independent_var'] = data['p'].apply(lambda x: eval(x)[independent_var_index])\n",
    "    \n",
    "    averaged_data = data.groupby('independent_var').agg({\n",
    "        'accuracy_error_normalised': 'mean',\n",
    "        'computation_time_normalised': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    independent_var_name = \"Volatility\" if independent_var_index == 1 else \"Time to Expiry\"\n",
    "\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    ax1.plot(averaged_data['independent_var'], averaged_data['accuracy_error_normalised'], color='blue', label='Average Accuracy Error')\n",
    "    ax1.set_title(f'Average Normalised Accuracy Error vs {independent_var_name}')\n",
    "    ax1.set_xlabel(str(independent_var_name))\n",
    "    ax1.set_ylabel('Average Normalised Accuracy Error')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(averaged_data['independent_var'], averaged_data['computation_time_normalised'], color='green', label='Average Computation Time')\n",
    "    ax2.set_title(f'Average Normalised Computation Time vs {independent_var_name}')\n",
    "    ax2.set_xlabel(str(independent_var_name))\n",
    "    ax2.set_ylabel('Average Normalised Computation Time')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_T = pd.read_csv('disso results/sensitivity_T.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_data(sensitivity_T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5045c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_vol = pd.read_csv('disso results/sensitivity_vol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_data(sensitivity_vol, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
